{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mu2S86VhS-8t"
   },
   "source": [
    "# GREMLIN_TF v2.1\n",
    "GREMLIN implemented in tensorflow\n",
    "\n",
    "### Change log:\n",
    "*   22Jan2018\n",
    " - moving [GREMLIN_TF_simple](https://colab.research.google.com/github/sokrypton/GREMLIN_CPP/blob/master/GREMLIN_TF_simple.ipynb) to a seperate notebook\n",
    "*   19Jan2018\n",
    " - in the past we found that optimizing V first, required less iterations for convergence. Since V can be computed exactly (assuming no W), we replace this first optimization step with a simple V initialization.\n",
    " - a few variables were renamed to be consistent with the c++ version\n",
    "*   16Jan2018\n",
    " - updating how indices are handled (for easier/cleaner parsing)\n",
    " - minor speed up in how we symmetrize and zero the diagional of W\n",
    "*   15Jan2018\n",
    " - LBFGS optimizer replaced with a modified version of the ADAM optimizer\n",
    " - Added option for stochastic gradient descent (via batch_size)\n",
    "  \n",
    "### Method:\n",
    "GREMLIN takes a multiple sequence alignment (MSA) and returns a Markov Random Field (MRF). The MRF consists of a one-body term (V) that encodes conservation, and a two-body term (W) that encodes co-evolution.\n",
    "\n",
    "For more details about the method see:\n",
    "[Google slides](https://docs.google.com/presentation/d/1aooxoksosSv7CWs9-ktqhUjyXR3wrgbG5a6PCr92od4/) and accompanying [Google colab](https://colab.research.google.com/drive/17RJcExuyifnd7ShTcsZGh6mBpWq0-s60)\n",
    "\n",
    "See [GREMLIN_TF_simple](https://colab.research.google.com/github/sokrypton/GREMLIN_CPP/blob/master/GREMLIN_TF_simple.ipynb) for a stripped down version of this code (with no funky gap removal, sequence weight, etc). This is intented for educational purpose,  and could also be very useful for anyone trying to modify or improve the algorithm!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yM3wyYU5SwYn"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# \"THE BEERWARE LICENSE\" (Revision 42):\n",
    "# <so@g.harvard.edu> and <pkk382@g.harvard.edu> wrote this code.\n",
    "# As long as you retain this notice, you can do whatever you want\n",
    "# with this stuff. If we meet someday, and you think this stuff\n",
    "# is worth it, you can buy us a beer in return.\n",
    "# --Sergey Ovchinnikov and Peter Koo\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLUvPVyxb7bo"
   },
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow==2.0.0-beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyJpLM_tJfrY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT, only tested using PYTHON 3!\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0Yp7bPRmvwU"
   },
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3c7KURqmugY"
   },
   "outputs": [],
   "source": [
    "################\n",
    "# note: if you are modifying the alphabet\n",
    "# make sure last character is \"-\" (gap)\n",
    "################\n",
    "alphabet = \"ARNDCQEGHILKMFPSTWYV-\"\n",
    "states = len(alphabet)\n",
    "a2n = {}\n",
    "for a,n in zip(alphabet,range(states)):\n",
    "  a2n[a] = n\n",
    "################\n",
    "\n",
    "def aa2num(aa):\n",
    "  '''convert aa into num'''\n",
    "  if aa in a2n: return a2n[aa]\n",
    "  else: return a2n['-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bX6GXKV3I2pm"
   },
   "source": [
    "## Functions for prepping the MSA (Multiple sequence alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zA0Bne59SUIu"
   },
   "outputs": [],
   "source": [
    "# from fasta\n",
    "def parse_fasta(filename,limit=-1):\n",
    "  '''function to parse fasta'''\n",
    "  header = []\n",
    "  sequence = []\n",
    "  lines = open(filename, \"r\")\n",
    "  for line in lines:\n",
    "    line = line.rstrip()\n",
    "    if line[0] == \">\":\n",
    "      if len(header) == limit:\n",
    "        break\n",
    "      header.append(line[1:])\n",
    "      sequence.append([])\n",
    "    else:\n",
    "      sequence[-1].append(line)\n",
    "  lines.close()\n",
    "  sequence = [''.join(seq) for seq in sequence]\n",
    "  return np.array(header), np.array(sequence)\n",
    "\n",
    "def filt_gaps(msa,gap_cutoff=0.5):\n",
    "  '''filters alignment to remove gappy positions'''\n",
    "  tmp = np.zeros_like(msa)\n",
    "  tmp[np.where(msa == 20)] = 1\n",
    "  non_gaps = np.where(np.sum(tmp.T,-1).T/msa.shape[0] < gap_cutoff)[0]\n",
    "  return msa[:,non_gaps],non_gaps\n",
    "\n",
    "def get_eff(msa,eff_cutoff=0.8):\n",
    "  '''compute effective weight for each sequence'''\n",
    "  ncol = msa.shape[1]\n",
    "  \n",
    "  # pairwise identity\n",
    "  msa_sm = 1.0 - squareform(pdist(msa,\"hamming\"))\n",
    "\n",
    "  # weight for each sequence\n",
    "  msa_w = np.zeros_like(msa_sm)\n",
    "  msa_w[np.where(msa_sm >= eff_cutoff)] = 1\n",
    "  msa_w = 1/np.sum(msa_w,-1)\n",
    "  \n",
    "  return msa_w\n",
    "\n",
    "def mk_msa(seqs):\n",
    "  '''converts list of sequences to msa'''\n",
    "  \n",
    "  msa_ori = []\n",
    "  for seq in seqs:\n",
    "    msa_ori.append([aa2num(aa) for aa in seq])\n",
    "  msa_ori = np.array(msa_ori)\n",
    "  \n",
    "  # remove positions with more than > 50% gaps\n",
    "  msa, v_idx = filt_gaps(msa_ori,0.5)\n",
    "  \n",
    "  # compute effective weight for each sequence\n",
    "  msa_weights = get_eff(msa,0.8)\n",
    "\n",
    "  # compute effective number of sequences\n",
    "  ncol = msa.shape[1] # length of sequence\n",
    "  w_idx = v_idx[np.stack(np.triu_indices(ncol,1),-1)]\n",
    "  \n",
    "  return {\"msa_ori\":msa_ori,\n",
    "          \"msa\":msa,\n",
    "          \"weights\":msa_weights,\n",
    "          \"neff\":np.sum(msa_weights),\n",
    "          \"v_idx\":v_idx,\n",
    "          \"w_idx\":w_idx,\n",
    "          \"nrow\":msa.shape[0],\n",
    "          \"ncol\":ncol,\n",
    "          \"ncol_ori\":msa_ori.shape[1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fky6gk-HlFyi"
   },
   "source": [
    "## GREMLIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gx14M7Tvu-Ct"
   },
   "outputs": [],
   "source": [
    "# external functions\n",
    "\n",
    "def sym_w(w):\n",
    "  '''symmetrize input matrix of shape (x,y,x,y)'''\n",
    "  x = w.shape[0]\n",
    "  w = w * np.reshape(1-np.eye(x),(x,1,x,1))\n",
    "  w = w + tf.transpose(w,[2,3,0,1])\n",
    "  return w\n",
    "\n",
    "def opt_adam(loss, name, var_list=None, lr=1.0, b1=0.9, b2=0.999, b_fix=False):\n",
    "  # adam optimizer\n",
    "  # Note: this is a modified version of adam optimizer. More specifically, we replace \"vt\"\n",
    "  # with sum(g*g) instead of (g*g). Furthmore, we find that disabling the bias correction\n",
    "  # (b_fix=False) speeds up convergence for our case.\n",
    "  \n",
    "  if var_list is None: var_list = tf.trainable_variables() \n",
    "  gradients = tf.gradients(loss,var_list)\n",
    "  if b_fix: t = tf.Variable(1.0,\"t\")\n",
    "  opt = []\n",
    "  for n,(x,g) in enumerate(zip(var_list,gradients)):\n",
    "    if g is not None:\n",
    "      ini = dict(initializer=tf.zeros_initializer,trainable=False)\n",
    "      mt = tf.get_variable(name+\"_mt_\"+str(n),shape=x.shape, **ini)\n",
    "      vt = tf.get_variable(name+\"_vt_\"+str(n),shape=(1,), **ini)\n",
    "      \n",
    "      mt_tmp = b1*mt+(1-b1)*g\n",
    "      vt_tmp = b2*vt+(1-b2)*tf.reduce_sum(tf.square(g))\n",
    "      lr_tmp = lr/tf.sqrt(vt_tmp)\n",
    "\n",
    "      if b_fix: lr_tmp = lr_tmp * tf.sqrt(1-tf.pow(b2,t))/(1-tf.pow(b1,t))\n",
    "\n",
    "      opt.append(x.assign_add(-lr_tmp * mt_tmp))\n",
    "      opt.append(vt.assign(vt_tmp))\n",
    "      opt.append(mt.assign(mt_tmp))\n",
    "        \n",
    "  if b_fix: t.assign_add(1.0)\n",
    "  return(tf.group(opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqYqlJAXVI9N"
   },
   "outputs": [],
   "source": [
    "def GREMLIN(msa, opt_type=\"adam\", opt_iter=100, opt_rate=1.0, batch_size=None):\n",
    "  \n",
    "  ##############################################################\n",
    "  # SETUP COMPUTE GRAPH\n",
    "  ##############################################################\n",
    "  # kill any existing tensorflow graph\n",
    "  tf.reset_default_graph()\n",
    "  \n",
    "  ncol = msa[\"ncol\"] # length of sequence\n",
    "\n",
    "  # msa (multiple sequence alignment) \n",
    "  MSA = tf.placeholder(tf.int32,shape=(None,ncol),name=\"msa\")\n",
    "  \n",
    "  # one-hot encode msa\n",
    "  OH_MSA = tf.one_hot(MSA,states)\n",
    "  \n",
    "  # msa weights\n",
    "  MSA_weights = tf.placeholder(tf.float32, shape=(None,), name=\"msa_weights\")\n",
    "  \n",
    "  # 1-body-term of the MRF\n",
    "  V = tf.get_variable(name=\"V\", \n",
    "                      shape=[ncol,states],\n",
    "                      initializer=tf.zeros_initializer)\n",
    "\n",
    "  # 2-body-term of the MRF\n",
    "  W = tf.get_variable(name=\"W\",\n",
    "                      shape=[ncol,states,ncol,states],\n",
    "                      initializer=tf.zeros_initializer)\n",
    "  \n",
    "  # symmetrize W\n",
    "  W = sym_w(W)\n",
    "  \n",
    "  def L2(x): return tf.reduce_sum(tf.square(x))\n",
    "  \n",
    "  ########################################\n",
    "  # V + W\n",
    "  ########################################\n",
    "  VW = V + tf.tensordot(OH_MSA,W,2)\n",
    "\n",
    "  # hamiltonian\n",
    "  H = tf.reduce_sum(tf.multiply(OH_MSA,VW),axis=2)\n",
    "  # local Z (parition function)\n",
    "  Z = tf.reduce_logsumexp(VW,axis=2)\n",
    "\n",
    "  # Psuedo-Log-Likelihood\n",
    "  PLL = tf.reduce_sum(H - Z, axis=1)\n",
    "\n",
    "  # Regularization\n",
    "  L2_V = 0.01 * L2(V)\n",
    "  L2_W = 0.01 * L2(W) * 0.5 * (ncol-1) * (states-1)\n",
    "\n",
    "  # loss function to minimize\n",
    "  loss = -tf.reduce_sum(PLL*MSA_weights)/tf.reduce_sum(MSA_weights)\n",
    "  loss = loss + (L2_V + L2_W)/msa[\"neff\"]\n",
    "\n",
    "  ##############################################################\n",
    "  # MINIMIZE LOSS FUNCTION\n",
    "  ##############################################################\n",
    "  if opt_type == \"adam\":  \n",
    "    opt = opt_adam(loss,\"adam\",lr=opt_rate)\n",
    "\n",
    "  # generate input/feed\n",
    "  def feed(feed_all=False):\n",
    "    if batch_size is None or feed_all:\n",
    "      return {MSA:msa[\"msa\"], MSA_weights:msa[\"weights\"]}\n",
    "    else:\n",
    "      idx = np.random.randint(0,msa[\"nrow\"],size=batch_size)\n",
    "      return {MSA:msa[\"msa\"][idx], MSA_weights:msa[\"weights\"][idx]}\n",
    "  \n",
    "  # optimize!\n",
    "  with tf.Session() as sess:\n",
    "    # initialize variables V and W\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # initialize V\n",
    "    msa_cat = tf.keras.utils.to_categorical(msa[\"msa\"],states)\n",
    "    pseudo_count = 0.01 * np.log(msa[\"neff\"])\n",
    "    V_ini = np.log(np.sum(msa_cat.T * msa[\"weights\"],-1).T + pseudo_count)\n",
    "    V_ini = V_ini - np.mean(V_ini,-1,keepdims=True)\n",
    "    sess.run(V.assign(V_ini))\n",
    "    \n",
    "    # compute loss across all data\n",
    "    get_loss = lambda: round(sess.run(loss,feed(feed_all=True)) * msa[\"neff\"],2)\n",
    "    print(\"starting\",get_loss())\n",
    "    \n",
    "    if opt_type == \"lbfgs\":\n",
    "      lbfgs = tf.contrib.opt.ScipyOptimizerInterface\n",
    "      opt = lbfgs(loss,method=\"L-BFGS-B\",options={'maxiter': opt_iter})\n",
    "      opt.minimize(sess,feed(feed_all=True))\n",
    "      \n",
    "    if opt_type == \"adam\":\n",
    "      for i in range(opt_iter):\n",
    "        sess.run(opt,feed())  \n",
    "        if (i+1) % int(opt_iter/10) == 0:\n",
    "          print(\"iter\",(i+1),get_loss())\n",
    "    \n",
    "    # save the V and W parameters of the MRF\n",
    "    V_ = sess.run(V)\n",
    "    W_ = sess.run(W)\n",
    "    \n",
    "  # only return upper-right triangle of matrix (since it's symmetric)\n",
    "  tri = np.triu_indices(ncol,1)\n",
    "  W_ = W_[tri[0],:,tri[1],:]\n",
    "  \n",
    "  mrf = {\"v\": V_,\n",
    "         \"w\": W_,\n",
    "         \"v_idx\": msa[\"v_idx\"],\n",
    "         \"w_idx\": msa[\"w_idx\"]}\n",
    "  \n",
    "  return mrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mppg0JLtP25z"
   },
   "source": [
    "## EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osaZwTSMOicF"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PREP MSA\n",
    "# ===============================================================================\n",
    "# parse fasta\n",
    "names, seqs = parse_fasta(\"1568390274.fas\")\n",
    "\n",
    "# process input sequences\n",
    "msa = mk_msa(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "CoBRuqmVVrbD",
    "outputId": "f004ad83-eb31-46a2-fb91-412189cdb992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "starting 512126.44\n",
      "iter 10 274547.91\n",
      "iter 20 259143.45\n",
      "iter 30 247316.87\n",
      "iter 40 244099.96\n",
      "iter 50 242917.38\n",
      "iter 60 242448.19\n",
      "iter 70 242270.73\n",
      "iter 80 242181.67\n",
      "iter 90 242135.14\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ===============================================================================\n",
    "# RUN GREMLIN\n",
    "# ===============================================================================\n",
    "# Note: the original GREMLIN uses the \"lbfgs\" optimizer which is EXTREMELY slow \n",
    "# in tensorflow. The modified adam optimizer is much faster, but may \n",
    "# require adjusting number of iterations (opt_iter) to converge to the same \n",
    "# solution. To switch back to the original, set opt_type=\"lbfgs\".\n",
    "# ===============================================================================\n",
    "mrf = GREMLIN(msa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCrfC2Um4xww"
   },
   "source": [
    "## Explore the contact map\n",
    "### Contact prediction:\n",
    "\n",
    "For contact prediction, the W matrix is reduced from LxLx21x21 to LxL matrix (by taking the L2norm for each of the 20x20). In the code below, you can access this as mtx[\"raw\"]. Further correction (average product correction) is then performed to the mtx[\"raw\"] to remove the effects of entropy, mtx[\"apc\"]. The relative ranking of mtx[\"apc\"] is used to assess importance. When there are enough effective sequences (>1000), we find that the top 1.0L contacts are ~90% accurate! When the number of effective sequences is lower, NN can help clean noise and fill in missing contacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxgQArVUPyPH"
   },
   "source": [
    "## Functions for extracting contacts from MRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMxp7up_P1_q"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "def normalize(x):\n",
    "  x = stats.boxcox(x - np.amin(x) + 1.0)[0]\n",
    "  x_mean = np.mean(x)\n",
    "  x_std = np.std(x)\n",
    "  return((x-x_mean)/x_std)\n",
    "\n",
    "def get_mtx(mrf):\n",
    "  '''get mtx given mrf'''\n",
    "  \n",
    "  # l2norm of 20x20 matrices (note: we ignore gaps)\n",
    "  raw = np.sqrt(np.sum(np.square(mrf[\"w\"][:,:-1,:-1]),(1,2)))\n",
    "  raw_sq = squareform(raw)\n",
    "\n",
    "  # apc (average product correction)\n",
    "  ap_sq = np.sum(raw_sq,0,keepdims=True)*np.sum(raw_sq,1,keepdims=True)/np.sum(raw_sq)\n",
    "  apc = squareform(raw_sq - ap_sq, checks=False)\n",
    "\n",
    "  mtx = {\"i\": mrf[\"w_idx\"][:,0],\n",
    "         \"j\": mrf[\"w_idx\"][:,1],\n",
    "         \"raw\": raw,\n",
    "         \"apc\": apc,\n",
    "         \"zscore\": normalize(apc)}\n",
    "  return mtx\n",
    "\n",
    "def plot_mtx(mtx,key=\"zscore\",vmin=1,vmax=3):\n",
    "  '''plot the mtx'''\n",
    "  plt.figure(figsize=(5,5))\n",
    "  plt.imshow(squareform(mtx[key]), cmap='Blues', interpolation='none', vmin=vmin, vmax=vmax)\n",
    "  plt.grid(False)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "RSleviAVPJ36",
    "outputId": "9e7d91a0-b83d-4fd4-c843-28128b53dc5d"
   },
   "outputs": [],
   "source": [
    "mtx = get_mtx(mrf)  \n",
    "plot_mtx(mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWaTHLTH5rGw"
   },
   "source": [
    "## Look at top co-evolving residue pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "9A2yeOJ8uPNM",
    "outputId": "65e9343b-3e2b-44d9-b6ed-6ac3ac03d042"
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# WARNING - WARNING - WARNING\n",
    "######################################################################################\n",
    "# - the i,j index starts at 0 (zero)\n",
    "# - the \"first\" position = 0\n",
    "# - often in biology first position of a sequence is 1\n",
    "#   for this index use i_aa and j_aa!\n",
    "\n",
    "# adding amino acid to index\n",
    "mtx[\"i_aa\"] = np.array([alphabet[msa['msa_ori'][0][i]]+\"_\"+str(i+1) for i in mtx[\"i\"]])\n",
    "mtx[\"j_aa\"] = np.array([alphabet[msa['msa_ori'][0][j]]+\"_\"+str(j+1) for j in mtx[\"j\"]])\n",
    "\n",
    "# load mtx into pandas dataframe\n",
    "pd_mtx = pd.DataFrame(mtx,columns=[\"i\",\"j\",\"apc\",\"zscore\",\"i_aa\",\"j_aa\"])\n",
    "\n",
    "# get contacts with sequence seperation > 5\n",
    "# sort by zscore, show top 10\n",
    "top = pd_mtx.loc[pd_mtx['j'] - pd_mtx['i'] > 5].sort_values(\"zscore\",ascending=False)\n",
    "top.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqG93dC12CKx"
   },
   "source": [
    "## Explore the MRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "k6BsheyNx3ID",
    "outputId": "6e4a2a37-1646-4ebe-cbbe-1814acceec5e"
   },
   "outputs": [],
   "source": [
    "def plot_v(mrf):  \n",
    "  al_a = list(alphabet)\n",
    "  v = mrf[\"v\"].T\n",
    "  mx = np.max((v.max(),np.abs(v.min())))\n",
    "  plt.figure(figsize=(v.shape[1]/4,states/4))\n",
    "  plt.imshow(-v,cmap='bwr',vmin=-mx,vmax=mx)\n",
    "  plt.xticks(np.arange(v.shape[1]))\n",
    "  plt.yticks(np.arange(0,21))\n",
    "  plt.grid(False)\n",
    "  ax = plt.gca()\n",
    "  ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x,y: mrf[\"v_idx\"][x])) \n",
    "  ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x,y: al_a[x]))\n",
    "  \n",
    "plot_v(mrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "colab_type": "code",
    "id": "OER2wALRvTkK",
    "outputId": "d35ee367-e597-4cac-c15d-0dd234004f3e"
   },
   "outputs": [],
   "source": [
    "def plot_w(mrf,i,j,i_aa,j_aa):\n",
    "  \n",
    "  n = int(np.where((mrf[\"w_idx\"][:,0] == i)&(mrf[\"w_idx\"][:,1] == j))[0])\n",
    "  w = mrf[\"w\"][n]\n",
    "  \n",
    "  with open(\"W_for_positions_\"+str(i_aa)+\"_\"+str(j_aa)+\".csv\",'w') as f:\n",
    "    f.write(\",\")\n",
    "    for k in alphabet:\n",
    "        f.write(k+\",\")\n",
    "    f.write(\"\\n\")\n",
    "    dummy = 0\n",
    "    for pos1 in w:\n",
    "        f.write(alphabet[dummy]+\",\")\n",
    "        for pos2 in pos1:\n",
    "            f.write(str(round(pos2,2))+\",\")\n",
    "        f.write(\"\\n\")\n",
    "        dummy += 1\n",
    "  mx = np.max((w.max(),np.abs(w.min())))\n",
    "  plt.figure(figsize=(states/4,states/4))\n",
    "  plt.imshow(-w,cmap='bwr',vmin=-mx,vmax=mx)\n",
    "  plt.xticks(np.arange(0,states))\n",
    "  plt.yticks(np.arange(0,states))\n",
    "  plt.grid(False)\n",
    "  \n",
    "  ax = plt.gca()\n",
    "  al_a = list(alphabet)\n",
    "  ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x,y: al_a[x])) \n",
    "  ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x,y: al_a[x]))\n",
    "  plt.title(f\"W for positions {i_aa} and {j_aa}\")\n",
    "  plt.savefig(f\"W for positions {i_aa} and {j_aa}.png\")\n",
    "  #plt.show()\n",
    "\n",
    "for n in range(50):\n",
    "  i = int(top.iloc[n][\"i\"])\n",
    "  j = int(top.iloc[n][\"j\"])\n",
    "  i_aa = top.iloc[n][\"i_aa\"]\n",
    "  j_aa = top.iloc[n][\"j_aa\"]\n",
    "  plot_w(mrf,i,j,i_aa,j_aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dw9TGERe3cw"
   },
   "source": [
    "## Useful input features for NN (Neural Networks)\n",
    "\n",
    "The \"apc\" values are typically used as input to the NN for contact cleaning or structure prediction. Though in recent advances (aka DeepMind/Alphafold), the entire MRF was used as the input. More specificially LxLx442. The 442 channels are the 21x21 + (raw and/or apc) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nqbDMKdsYG7Q",
    "outputId": "6ee8dafa-51f6-4f8d-e201-d2bf585aaf9f"
   },
   "outputs": [],
   "source": [
    "w_out = np.zeros((msa[\"ncol_ori\"],msa[\"ncol_ori\"],442))\n",
    "v_out = np.zeros((msa[\"ncol_ori\"],21))\n",
    "\n",
    "mrf_ = np.reshape(mrf[\"w\"],(-1,441))\n",
    "mtx_ = np.expand_dims(mtx[\"apc\"],-1)\n",
    "\n",
    "w_out[(mtx[\"i\"],mtx[\"j\"])] = np.concatenate((mrf_,mtx_),-1)\n",
    "w_out += np.transpose(w_out,(1,0,2))\n",
    "v_out[mrf[\"v_idx\"]] = mrf[\"v\"]\n",
    "\n",
    "print(\"w_out\",w_out.shape)\n",
    "print(\"v_out\",v_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip files.zip *png *csv *ipynb *fas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *png *csv *fas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GREMLIN_TF_v2_for_per.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
